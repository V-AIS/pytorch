{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as opti\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import PIL\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "config['random']=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(img_path, resize=None):\n",
    "    img = Image.open(img_path)\n",
    "    if resize:\n",
    "        img = img.resize(resize,PIL.Image.BICUBIC)\n",
    "    return img\n",
    "def preproc4torch(img):\n",
    "    mean = np.array([[[0.485, 0.456, 0.406]]])\n",
    "    std = np.array([[[0.229, 0.224, 0.225]]])\n",
    "    result = np.array(img)/255.\n",
    "    result = np.flip(result, axis=2)\n",
    "    result = np.transpose((result-mean)/std, [2,0,1])\n",
    "    result = torch.Tensor(result).unsqueeze(0)\n",
    "    return result.to(device)\n",
    "\n",
    "def deproc4plot(img):\n",
    "    mean = np.array([[[0.485, 0.456, 0.406]]])\n",
    "    std = np.array([[[0.229, 0.224, 0.225]]])\n",
    "    result = img.detach().cpu().squeeze().numpy()\n",
    "    result = np.transpose(result, [1,2,0])\n",
    "    result = (result*std + mean)*255.\n",
    "    result = np.clip(result, 0, 255)\n",
    "    result = np.flip(result, axis=2)\n",
    "    return np.uint8(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Extractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Extractor, self).__init__()\n",
    "        self.style_idx = ['0', '5', '10', '19', '28'] \n",
    "        self.content_idx = ['20']\n",
    "        self.extractor = models.vgg19(pretrained=True).features\n",
    "        \n",
    "    def forward(self, x, mode = None):\n",
    "        \"\"\"Extract multiple convolutional feature maps.\"\"\"\n",
    "        assert mode, \"Please input mode of Extractor\"\n",
    "        if mode == 'content':feature_idx = self.content_idx\n",
    "        else: feature_idx = self.style_idx\n",
    "        features = []\n",
    "        for num, layer in self.extractor.named_children():\n",
    "            x = layer(x)\n",
    "            if num in feature_idx:\n",
    "                features.append(x)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare each images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_img = load_img('../cat.jpg')\n",
    "style_img = load_img('../starry_night.jpg', content_img.size)\n",
    "\n",
    "content_img = preproc4torch(content_img)\n",
    "style_img = preproc4torch(style_img)\n",
    "print('Content image shape : ', content_img.shape)\n",
    "print('Style image shape : ', style_img.shape)\n",
    "\n",
    "if config['random']:\n",
    "    target_img = Variable(torch.randn(content_img.size()), requires_grad=True).to(device)\n",
    "else:\n",
    "    target_img = Variable(content_img.data.clone(), requires_grad=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = Extractor().to(device).eval()\n",
    "\n",
    "optim = torch.optim.Adam([target_img], lr=0.001, betas=[0.5, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Content_Loss(content, target):\n",
    "    return torch.mean((content[0] - target[0])**2)\n",
    "\n",
    "def Style_Loss(style, target):\n",
    "    loss = 0\n",
    "    for s_f, t_f in zip(style, target):\n",
    "        b, c, h, w = s_f.size()\n",
    "        s_f = s_f.view(b, c, h*w)\n",
    "        t_f = t_f.view(b, c, h*w)\n",
    "        \n",
    "        s_f = torch.bmm(s_f, s_f.transpose(1,2))\n",
    "        t_f = torch.bmm(t_f, t_f.transpose(1,2))\n",
    "        loss += torch.mean((s_f - t_f)**2) / (c**2) \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 5000\n",
    "for step in tqdm.tqdm(range(steps)):\n",
    "    \n",
    "    content = extractor(content_img, 'content')\n",
    "    style = extractor(style_img, 'style')\n",
    "    target_content = extractor(target_img, 'content')\n",
    "    target_style = extractor(target_img, 'style')\n",
    "    \n",
    "    c_loss = Content_Loss(content, target_content)\n",
    "    s_loss = Style_Loss(style, target_style)\n",
    "    \n",
    "    loss = c_loss + 100*s_loss\n",
    "    \n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "        \n",
    "make = deproc4plot(target_img)\n",
    "plt.imshow(make)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
